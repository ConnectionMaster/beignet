/* 
 * Copyright Â© 2012 Intel Corporation
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License as published by the Free Software Foundation; either
 * version 2 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library. If not, see <http://www.gnu.org/licenses/>.
 *
 * Author: Benjamin Segovia <benjamin.segovia@intel.com>
 */

#include "string"
namespace gbe {
std::string sim_vector_str = 
"/*\n"
" * Copyright 2012 Intel Corporation\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a\n"
" * copy of this software and associated documentation files (the \"Software\"),\n"
" * to deal in the Software without restriction, including without limitation\n"
" * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n"
" * and/or sell copies of the Software, and to permit persons to whom the\n"
" * Software is furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice (including the next\n"
" * paragraph) shall be included in all copies or substantial portions of the\n"
" * Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n"
" * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n"
" * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n"
" * DEALINGS IN THE SOFTWARE.\n"
" */\n"
"\n"
"/**\n"
" * \\file sim_vector.h\n"
" * \\author Benjamin Segovia <benjamin.segovia@intel.com>\n"
" *\n"
" * c++ class helper for the simulator\n"
" */\n"
"\n"
"#ifndef __GBE_SIM_VECTOR_H__\n"
"#define __GBE_SIM_VECTOR_H__\n"
"\n"
"#include <xmmintrin.h>\n"
"#include <emmintrin.h>\n"
"#include <pmmintrin.h>\n"
"#include <smmintrin.h>\n"
"#include <stdint.h>\n"
"#include <cmath>\n"
"\n"
"#define INLINE inline __attribute__((always_inline))\n"
"#define ID(X) (X)\n"
"#define PS2SI(X) _mm_castps_si128(X)\n"
"#define SI2PS(X) _mm_castsi128_ps(X)\n"
"\n"
"/* Some extra SSE functions */\n"
"template<size_t i0, size_t i1, size_t i2, size_t i3>\n"
"INLINE const __m128 shuffle(const __m128& b) {\n"
"  return _mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(b), _MM_SHUFFLE(i3, i2, i1, i0)));\n"
"}\n"
"template<size_t i> INLINE\n"
"__m128 expand(const __m128& b) { \n"
"  return shuffle<i, i, i, i>(b);\n"
"}\n"
"template<size_t index_0, size_t index_1, size_t index_2, size_t index_3>\n"
"INLINE const __m128i shuffle(const __m128i& a) {\n"
"  return _mm_shuffle_epi32(a, _MM_SHUFFLE(index_3, index_2, index_1, index_0));\n"
"}\n"
"template<size_t index>\n"
"INLINE const __m128i expand(const __m128i& b) {\n"
"  return shuffle<index, index, index, index>(b);\n"
"}\n"
"\n"
"/*! Base structure for scalar double word (32 bits) */\n"
"union scalar_dw {\n"
"  INLINE scalar_dw(void) {}\n"
"  INLINE scalar_dw(uint32_t u) { this->u = u; }\n"
"  INLINE scalar_dw(int32_t s) { this->s = s; }\n"
"  INLINE scalar_dw(float f) { this->f = f; }\n"
"  uint32_t u; int32_t s; float f;\n"
"};\n"
"\n"
"/*! Base structure for scalar word (16 bits) */\n"
"union scalar_w {\n"
"  INLINE scalar_w(void) {}\n"
"  INLINE scalar_w(uint16_t u) { this->u = u; }\n"
"  INLINE scalar_w(int16_t s) { this->s = s; }\n"
"  INLINE float toFloat(void) const {\n"
"    union {uint16_t u[2]; float f;} x;\n"
"    x.u[0] = u;\n"
"    x.u[1] = 0;\n"
"    return x.f;\n"
"  }\n"
"  uint16_t u; int16_t s;\n"
"};\n"
"\n"
"/*! Base structure for scalar mask */\n"
"union scalar_m { uint32_t u; int32_t s; float f; };\n"
"\n"
"/*! Base structure for vectors 4 / 8 / 16 / 32 double words */\n"
"template <uint32_t vectorNum>\n"
"struct simd_dw {\n"
"  INLINE simd_dw(void) {}\n"
"  INLINE simd_dw(const scalar_dw &s) {\n"
"    for (uint32_t i = 0; i < vectorNum; ++i) m[i] = _mm_load1_ps(&s.f);\n"
"  }\n"
"  simd_dw &operator= (const scalar_dw &s) {\n"
"    for (uint32_t i = 0; i < vectorNum; ++i) m[i] = _mm_load1_ps(&s.f);\n"
"    return *this;\n"
"  }\n"
"  __m128 m[vectorNum];\n"
"};\n"
"\n"
"/*! Base structure for vectors 4 / 8 / 16 / 32 words. We do not store 8 shorts\n"
" *  but only 4. This makes everything much simpler even if it is clearly slower\n"
" */\n"
"template <uint32_t vectorNum>\n"
"struct simd_w {\n"
"  INLINE simd_w(void) {}\n"
"  INLINE simd_w(const scalar_w &s) {\n"
"    const float f = s.toFloat();\n"
"    for (uint32_t i = 0; i < vectorNum; ++i) m[i] = _mm_load1_ps(&f);\n"
"  }\n"
"  simd_w &operator= (const scalar_w &s) {\n"
"    const float f = s.toFloat();\n"
"    for (uint32_t i = 0; i < vectorNum; ++i) m[i] = _mm_load1_ps(&f);\n"
"    return *this;\n"
"  }\n"
"  __m128 m[vectorNum];\n"
"};\n"
"\n"
"/*! Base structure for 4 / 8 / 16 / 32 booleans (m stands for \"mask\") */\n"
"template <uint32_t vectorNum>\n"
"struct simd_m {\n"
"  INLINE simd_m(void) {}\n"
"  INLINE simd_m(scalar_m s) {\n"
"    for (uint32_t i = 0; i < vectorNum; ++i) m[i] = _mm_load1_ps(&s.f);\n"
"  }\n"
"  __m128 m[vectorNum];\n"
"};\n"
"\n"
"/*! Select instruction on vectors */\n"
"template <uint32_t vectorNum>\n"
"INLINE void select(simd_dw<vectorNum> &dst,\n"
"                   const simd_dw<vectorNum> &src0,\n"
"                   const simd_dw<vectorNum> &src1,\n"
"                   const simd_m<vectorNum> &mask)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_blendv_ps(src0.m[i], src1.m[i], mask.m[i]);\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void select(simd_m<vectorNum> &dst,\n"
"                   const simd_m<vectorNum> &src0,\n"
"                   const simd_m<vectorNum> &src1,\n"
"                   const simd_m<vectorNum> &mask)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_blendv_ps(src0.m[i], src1.m[i], mask.m[i]);\n"
"}\n"
"\n"
"/*! To cast through memory 32 bits values in sse registers */\n"
"union cast_dw {\n"
"  INLINE cast_dw(uint32_t u0, uint32_t u1, uint32_t u2, uint32_t u3) {\n"
"    u[0] = u0; u[1] = u1; u[2] = u2; u[3] = u3;\n"
"  }\n"
"  INLINE cast_dw(int32_t s0, int32_t s1, int32_t s2, int32_t s3) {\n"
"    s[0] = s0; s[1] = s1; s[2] = s2; s[3] = s3;\n"
"  }\n"
"  INLINE cast_dw(float f0, float f1, float f2, float f3) {\n"
"    f[0] = f0; f[1] = f1; f[2] = f2; f[3] = f3;\n"
"  }\n"
"  INLINE cast_dw(const __m128 &v) : v(v) {}\n"
"  INLINE cast_dw(const __m128i &vi) : vi(vi) {}\n"
"  INLINE cast_dw(void) {}\n"
"  __m128 v;\n"
"  __m128i vi;\n"
"  uint32_t u[4];\n"
"  int32_t s[4];\n"
"  float f[4];\n"
"};\n"
"static const cast_dw allTrue(0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff);\n"
"\n"
"/*! To cast through memory 16 bits values in sse registers */\n"
"union cast_w {\n"
"  INLINE cast_w(int16_t s0, int16_t s1, int16_t s2, int16_t s3) {\n"
"    s[0].v = s0; s[1].v = s1; s[2].v = s2; s[3].v = s3;\n"
"    s[0].pad = s[1].pad = s[2].pad = s[3].pad = 0;\n"
"  }\n"
"  INLINE cast_w(uint16_t u0, uint16_t u1, uint16_t u2, uint16_t u3) {\n"
"    u[0].v = u0; u[1].v = u1; u[2].v = u2; u[3].v = u3;\n"
"    u[0].pad = u[1].pad = u[2].pad = u[3].pad = 0;\n"
"  }\n"
"  INLINE cast_w(const __m128 &v) : v(v) {}\n"
"  INLINE cast_w(const __m128i &vi) : vi(vi) {}\n"
"  INLINE cast_w(void) {}\n"
"  __m128 v;\n"
"  __m128i vi;\n"
"  struct { uint16_t v; uint16_t pad; } u[4];\n"
"  struct {  int16_t v;  int16_t pad; } s[4];\n"
"};\n"
"\n"
"/*! Make a mask true */\n"
"template <uint32_t vectorNum>\n"
"INLINE void allTrueMask(simd_m<vectorNum> &x) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) x.m[i] = allTrue.v;\n"
"}\n"
"\n"
"/* Some convenient typedefs */\n"
"typedef scalar_dw  simd1dw;\n"
"typedef simd_dw<1> simd4dw;\n"
"typedef simd_dw<2> simd8dw;\n"
"typedef simd_dw<4> simd16dw;\n"
"typedef simd_dw<8> simd32dw;\n"
"typedef scalar_w   simd1w;\n"
"typedef simd_w<1>  simd4w;\n"
"typedef simd_w<2>  simd8w;\n"
"typedef simd_w<4>  simd16w;\n"
"typedef simd_w<8>  simd32w;\n"
"typedef scalar_m   simd1m;\n"
"typedef simd_m<1>  simd4m;\n"
"typedef simd_m<2>  simd8m;\n"
"typedef simd_m<4>  simd16m;\n"
"typedef simd_m<8>  simd32m;\n"
"\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"// Vector instructions\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"/* Simple function to get the number of element per vector */\n"
"template <uint32_t vectorNum>\n"
"INLINE uint32_t elemNum(const simd_dw<vectorNum> &x) {\n"
"  return 4*vectorNum;\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE uint32_t elemNum(const simd_m<vectorNum> &x) {\n"
"  return 4*vectorNum;\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE uint32_t elemNum(const simd_w<vectorNum> &x) {\n"
"  return 4*vectorNum;\n"
"}\n"
"\n"
"/* Build an integer mask from the mask vectors */\n"
"template <uint32_t vectorNum>\n"
"INLINE uint32_t mask(const simd_m<vectorNum> v) {\n"
"  uint32_t m = _mm_movemask_ps(v.m[0]);\n"
"  for (uint32_t i = 1; i < vectorNum; ++i)\n"
"    m |= (_mm_movemask_ps(v.m[i]) << (4*i));\n"
"  return m;\n"
"}\n"
"\n"
"/* MOV instruction */\n"
"template <uint32_t vectorNum>\n"
"INLINE void MOV_S32(simd_dw<vectorNum> &dst, const simd_dw<vectorNum> &v) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) dst.m[i] = v.m[i];\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MOV_S32(simd_dw<vectorNum> &dst, const scalar_dw &x) {\n"
"  const __m128 v = _mm_load1_ps(&x.f);\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) dst.m[i] = v;\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MOV_S16(simd_w<vectorNum> &dst, const simd_w<vectorNum> &v) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) dst.m[i] = v.m[i];\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MOV_S16(simd_w<vectorNum> &dst, const scalar_w &x) {\n"
"  const float f = x.toFloat();\n"
"  const __m128 v = _mm_load1_ps(&f);\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) dst.m[i] = v;\n"
"}\n"
"\n"
"/* Vector instructions that use sse* */\n"
"#define VEC_OP(DST_TYPE, SRC_TYPE, SCALAR_TYPE, NAME, INTRINSIC_NAME, FN, FN0, FN1)\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\\\n"
"    dst.m[i] = FN(INTRINSIC_NAME(FN0(v0.m[i]), FN1(v1.m[i])));\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, v0, SRC_TYPE(v1));\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), v1);\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), SRC_TYPE(v1));\\\n"
"}\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, ADD_F, _mm_add_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, SUB_F, _mm_sub_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, MUL_F, _mm_mul_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, DIV_F, _mm_div_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, EQ_F, _mm_cmpeq_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, NE_F, _mm_cmpneq_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, LT_F, _mm_cmplt_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, LE_F, _mm_cmple_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, GT_F, _mm_cmpgt_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, GE_F, _mm_cmpge_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, ADD_S32, _mm_add_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, SUB_S32, _mm_sub_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, EQ_S32, _mm_cmpeq_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, LT_S32, _mm_cmplt_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_m<vectorNum>,  simd_dw<vectorNum>, scalar_dw, GT_S32, _mm_cmpgt_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, AND_S32, _mm_and_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, OR_S32, _mm_or_ps, ID, ID, ID);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, XOR_S32, _mm_xor_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_w<vectorNum>,  scalar_w,  EQ_S16, _mm_cmpeq_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w, ADD_S16, _mm_add_epi16, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w, SUB_S16, _mm_sub_epi16, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w, AND_S16, _mm_and_ps, ID, ID, ID);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w, OR_S16, _mm_or_ps, ID, ID, ID);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w, XOR_S16, _mm_xor_ps, ID, ID, ID);\n"
"VEC_OP(simd_m<vectorNum>,  simd_m<vectorNum>,  scalar_m, AND_M,   _mm_and_ps, ID, ID, ID);\n"
"#undef VEC_OP\n"
"\n"
"/* Vector integer operations that we can get by switching argument order */\n"
"#define VEC_OP(DST_TYPE, SRC_TYPE, NAME, INTRINSIC_NAME, FN, FN0, FN1)\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\\\n"
"    dst.m[i] = _mm_xor_ps(FN(INTRINSIC_NAME(FN1(v0.m[i]), FN0(v1.m[i]))), allTrue.v);\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const scalar_dw &v1) {\\\n"
"  NAME(dst, v0, simd_dw<vectorNum>(v1));\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const scalar_dw &v0, const SRC_TYPE &v1) {\\\n"
"  NAME(dst, simd_dw<vectorNum>(v0), v1);\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const scalar_dw &v0, const scalar_dw &v1) {\\\n"
"  NAME(dst, simd_dw<vectorNum>(v0), simd_dw<vectorNum>(v1));\\\n"
"}\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, GE_S32, _mm_cmplt_epi32, SI2PS, PS2SI, PS2SI);\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, LE_S32, _mm_cmpgt_epi32, SI2PS, PS2SI, PS2SI);\n"
"#undef VEC_OP\n"
"\n"
"/* Vector binary integer operations that require C */\n"
"#define VEC_OP(DST_TYPE, SRC_TYPE, SCALAR_TYPE, CAST_TYPE, NAME, OP, FIELD)\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\\\n"
"    CAST_TYPE c0(v0.m[i]), c1(v1.m[i]), d;\\\n"
"    for (uint32_t j = 0; j < 4; ++j)\\\n"
"      d.FIELD = c0.FIELD OP c1.FIELD;\\\n"
"    dst.m[i] = d.v;\\\n"
"  }\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, v0, SRC_TYPE(v1));\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), v1);\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), SRC_TYPE(v1));\\\n"
"}\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, MUL_S32, *, s[j]);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, DIV_S32, /, s[j]);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, REM_S32, %, s[j]);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, MUL_U32, *, u[j]);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, DIV_U32, /, u[j]);\n"
"VEC_OP(simd_dw<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, REM_U32, %, u[j]);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  MUL_S16, *, s[j].v);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  DIV_S16, /, s[j].v);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  REM_S16, %, s[j].v);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  MUL_U16, *, u[j].v);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  DIV_U16, /, u[j].v);\n"
"VEC_OP(simd_w<vectorNum>,  simd_w<vectorNum>,  scalar_w,  cast_w,  REM_U16, %, u[j].v);\n"
"#undef VEC_OP\n"
"\n"
"/* Vector compare vectors that require C */\n"
"#define VEC_OP(DST_TYPE, SRC_TYPE, SCALAR_TYPE, CAST_TYPE, NAME, OP, FIELD)\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\\\n"
"    CAST_TYPE c0(v0.m[i]), c1(v1.m[i]);\\\n"
"    cast_dw d;\\\n"
"    for (uint32_t j = 0; j < 4; ++j)\\\n"
"      d.u[j] = (c0.FIELD OP c1.FIELD) ? ~0u : 0u;\\\n"
"    dst.m[i] = d.v;\\\n"
"  }\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SRC_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, v0, SRC_TYPE(v1));\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SRC_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), v1);\\\n"
"}\\\n"
"template <uint32_t vectorNum>\\\n"
"INLINE void NAME(DST_TYPE &dst, const SCALAR_TYPE &v0, const SCALAR_TYPE &v1) {\\\n"
"  NAME(dst, SRC_TYPE(v0), SRC_TYPE(v1));\\\n"
"}\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, LE_U32, <=, u[j]);\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, LT_U32, <, u[j]);\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, GE_U32, >=, u[j]);\n"
"VEC_OP(simd_m<vectorNum>, simd_dw<vectorNum>, scalar_dw, cast_dw, GT_U32, >, u[j]);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  LE_U16, <=, u[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  LT_U16, <, u[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  GE_U16, >=, u[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  GT_U16, >, u[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  LE_S16, <=, s[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  LT_S16, <, s[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  GE_S16, >=, s[j].v);\n"
"VEC_OP(simd_m<vectorNum>, simd_w<vectorNum>,  scalar_w,  cast_w,  GT_S16, >, s[j].v);\n"
"#undef VEC_OP\n"
"\n"
"/* Get NE from EQ */\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S32(simd_m<vectorNum> &dst,\n"
"                   const simd_dw<vectorNum> &v0,\n"
"                   const simd_dw<vectorNum> &v1)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_xor_ps(allTrue.v, SI2PS(_mm_cmpeq_epi32(PS2SI(v0.m[i]), PS2SI(v1.m[i]))));\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S32(simd_m<vectorNum> &dst,\n"
"                   const simd_dw<vectorNum> &v0,\n"
"                   const scalar_dw &v1)\n"
"{\n"
"  NE_S32(dst, v0, simd_dw<vectorNum>(v1));\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S32(simd_m<vectorNum> &dst,\n"
"                   const scalar_dw &v0,\n"
"                   const simd_dw<vectorNum> &v1)\n"
"{\n"
"  NE_S32(dst, simd_dw<vectorNum>(v0), v1);\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S16(simd_m<vectorNum> &dst,\n"
"                   const simd_w<vectorNum> &v0,\n"
"                   const simd_w<vectorNum> &v1)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_xor_ps(allTrue.v, SI2PS(_mm_cmpeq_epi32(PS2SI(v0.m[i]), PS2SI(v1.m[i]))));\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S16(simd_m<vectorNum> &dst,\n"
"                   const simd_w<vectorNum> &v0,\n"
"                   const scalar_w &v1)\n"
"{\n"
"  NE_S16(dst, v0, simd_w<vectorNum>(v1));\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S16(simd_m<vectorNum> &dst,\n"
"                   const scalar_w &v0,\n"
"                   const simd_w<vectorNum> &v1)\n"
"{\n"
"  NE_S16(dst, simd_w<vectorNum>(v0), v1);\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void NE_S16(simd_m<vectorNum> &dst,\n"
"                   const scalar_w &v0,\n"
"                   const scalar_w &v1)\n"
"{\n"
"  NE_S16(dst, simd_w<vectorNum>(v0), simd_w<vectorNum>(v1));\n"
"}\n"
"\n"
"/* Load from contiguous double words */\n"
"template <uint32_t vectorNum>\n"
"INLINE void LOAD(simd_dw<vectorNum> &dst, const char *ptr) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_loadu_ps((const float*) ptr + 4*i);\n"
"}\n"
"\n"
"/* Store to contiguous double words */\n"
"template <uint32_t vectorNum>\n"
"INLINE void STORE(const simd_dw<vectorNum> &src, char *ptr) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    _mm_storeu_ps((float*) ptr + 4*i, src.m[i]);\n"
"}\n"
"\n"
"/* Load from contiguous words */\n"
"template <uint32_t vectorNum>\n"
"INLINE void LOAD(simd_w<vectorNum> &dst, const char *ptr) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const uint16_t u0 = *((uint16_t*) ptr + 4*i + 0);\n"
"    const uint16_t u1 = *((uint16_t*) ptr + 4*i + 1);\n"
"    const uint16_t u2 = *((uint16_t*) ptr + 4*i + 2);\n"
"    const uint16_t u3 = *((uint16_t*) ptr + 4*i + 3);\n"
"    const cast_w w(u0,u1,u2,u3);\n"
"    dst.m[i] = w.v;\n"
"  }\n"
"}\n"
"\n"
"/* Store to contiguous words */\n"
"template <uint32_t vectorNum>\n"
"INLINE void STORE(const simd_w<vectorNum> &src, char *ptr) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const cast_w w(src.m[i]);\n"
"    *((uint16_t*) ptr + 4*i + 0) = w.u[0].v;\n"
"    *((uint16_t*) ptr + 4*i + 1) = w.u[1].v;\n"
"    *((uint16_t*) ptr + 4*i + 2) = w.u[2].v;\n"
"    *((uint16_t*) ptr + 4*i + 3) = w.u[3].v;\n"
"  }\n"
"}\n"
"\n"
"/* Load immediates */\n"
"template <uint32_t vectorNum>\n"
"INLINE void LOADI(simd_dw<vectorNum> &dst, uint32_t u) {\n"
"  union { uint32_t u; float f; } cast;\n"
"  cast.u = u;\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    dst.m[i] = _mm_load1_ps(&cast.f);\n"
"}\n"
"\n"
"/* Scatter */\n"
"template <uint32_t vectorNum>\n"
"INLINE void SCATTER(const simd_dw<vectorNum> &offset,\n"
"                    const simd_dw<vectorNum> &value,\n"
"                    char *base_address) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const int v0 = _mm_extract_epi32(PS2SI(value.m[i]), 0);\n"
"    const int v1 = _mm_extract_epi32(PS2SI(value.m[i]), 1);\n"
"    const int v2 = _mm_extract_epi32(PS2SI(value.m[i]), 2);\n"
"    const int v3 = _mm_extract_epi32(PS2SI(value.m[i]), 3);\n"
"    const int o0 = _mm_extract_epi32(PS2SI(offset.m[i]), 0);\n"
"    const int o1 = _mm_extract_epi32(PS2SI(offset.m[i]), 1);\n"
"    const int o2 = _mm_extract_epi32(PS2SI(offset.m[i]), 2);\n"
"    const int o3 = _mm_extract_epi32(PS2SI(offset.m[i]), 3);\n"
"    *(int*)(base_address + o0) = v0;\n"
"    *(int*)(base_address + o1) = v1;\n"
"    *(int*)(base_address + o2) = v2;\n"
"    *(int*)(base_address + o3) = v3;\n"
"  }\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void SCATTER(const simd_dw<vectorNum> &offset,\n"
"                    const scalar_dw &value,\n"
"                    char *base_address) {\n"
"  SCATTER(offset, simd_dw<vectorNum>(value), base_address);\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void SCATTER(const scalar_dw &offset,\n"
"                    const simd_dw<vectorNum> &value,\n"
"                    char *base_address) {\n"
"  SCATTER(simd_dw<vectorNum>(offset), value, base_address);\n"
"}\n"
"\n"
"/* Masked scatter will only store unmasked lanes */\n"
"template <uint32_t vectorNum>\n"
"INLINE void MASKED_SCATTER(const simd_dw<vectorNum> &offset,\n"
"                           const simd_dw<vectorNum> &value,\n"
"                           char *base_address,\n"
"                           uint32_t mask)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const int v0 = _mm_extract_epi32(PS2SI(value.m[i]), 0);\n"
"    const int v1 = _mm_extract_epi32(PS2SI(value.m[i]), 1);\n"
"    const int v2 = _mm_extract_epi32(PS2SI(value.m[i]), 2);\n"
"    const int v3 = _mm_extract_epi32(PS2SI(value.m[i]), 3);\n"
"    const int o0 = _mm_extract_epi32(PS2SI(offset.m[i]), 0);\n"
"    const int o1 = _mm_extract_epi32(PS2SI(offset.m[i]), 1);\n"
"    const int o2 = _mm_extract_epi32(PS2SI(offset.m[i]), 2);\n"
"    const int o3 = _mm_extract_epi32(PS2SI(offset.m[i]), 3);\n"
"    if (mask & 1) *(int*)(base_address + o0) = v0;\n"
"    if (mask & 2) *(int*)(base_address + o1) = v1;\n"
"    if (mask & 4) *(int*)(base_address + o2) = v2;\n"
"    if (mask & 8) *(int*)(base_address + o3) = v3;\n"
"    mask = mask >> 4;\n"
"  }\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MASKED_SCATTER(const simd_dw<vectorNum> &offset,\n"
"                           const scalar_dw &value,\n"
"                           char *base_address,\n"
"                           uint32_t mask)\n"
"{\n"
"  MASKED_SCATTER(offset, simd_dw<vectorNum>(value), base_address, mask);\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MASKED_SCATTER(const scalar_dw &offset,\n"
"                           const simd_dw<vectorNum> &value,\n"
"                           char *base_address,\n"
"                           uint32_t mask)\n"
"{\n"
"  MASKED_SCATTER(simd_dw<vectorNum>(offset), value, base_address, mask);\n"
"}\n"
"\n"
"/* Gather */\n"
"template <uint32_t vectorNum>\n"
"INLINE void GATHER(simd_dw<vectorNum> &dst,\n"
"                   const simd_dw<vectorNum> &offset,\n"
"                   const char *base_address) {\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const int o0 = _mm_extract_epi32(PS2SI(offset.m[i]) , 0);\n"
"    const int o1 = _mm_extract_epi32(PS2SI(offset.m[i]), 1);\n"
"    const int o2 = _mm_extract_epi32(PS2SI(offset.m[i]), 2);\n"
"    const int o3 = _mm_extract_epi32(PS2SI(offset.m[i]), 3);\n"
"    const int v0 = *(const int*)(base_address + o0);\n"
"    const int v1 = *(const int*)(base_address + o1);\n"
"    const int v2 = *(const int*)(base_address + o2);\n"
"    const int v3 = *(const int*)(base_address + o3);\n"
"    dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v0, 0));\n"
"    dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v1, 1));\n"
"    dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v2, 2));\n"
"    dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v3, 3));\n"
"  }\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void GATHER(simd_dw<vectorNum> &dst,\n"
"                   const scalar_dw &offset,\n"
"                   const char *base_address) {\n"
"  GATHER(dst, simd_dw<vectorNum>(offset), base_address);\n"
"}\n"
"\n"
"/* Masked gather will only load activated lanes */\n"
"template <uint32_t vectorNum>\n"
"INLINE void MASKED_GATHER(simd_dw<vectorNum> &dst,\n"
"                          const simd_dw<vectorNum> &offset,\n"
"                          const char *base_address,\n"
"                          uint32_t mask)\n"
"{\n"
"  for (uint32_t i = 0; i < vectorNum; ++i) {\n"
"    const int o0 = _mm_extract_epi32(PS2SI(offset.m[i]) , 0);\n"
"    const int o1 = _mm_extract_epi32(PS2SI(offset.m[i]), 1);\n"
"    const int o2 = _mm_extract_epi32(PS2SI(offset.m[i]), 2);\n"
"    const int o3 = _mm_extract_epi32(PS2SI(offset.m[i]), 3);\n"
"    const int v0 = *(const int*)(base_address + o0);\n"
"    const int v1 = *(const int*)(base_address + o1);\n"
"    const int v2 = *(const int*)(base_address + o2);\n"
"    const int v3 = *(const int*)(base_address + o3);\n"
"    if (mask & 1) dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v0, 0));\n"
"    if (mask & 2) dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v1, 1));\n"
"    if (mask & 4) dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v2, 2));\n"
"    if (mask & 8) dst.m[i] = SI2PS(_mm_insert_epi32(PS2SI(dst.m[i]), v3, 3));\n"
"    mask = mask >> 4;\n"
"  }\n"
"}\n"
"template <uint32_t vectorNum>\n"
"INLINE void MASKED_GATHER(simd_dw<vectorNum> &dst,\n"
"                          const scalar_dw &offset,\n"
"                          const char *base_address,\n"
"                          uint32_t mask)\n"
"{\n"
"  MASKED_GATHER(dst, simd_dw<vectorNum>(offset), base_address, mask);\n"
"}\n"
"\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"// Scalar instructions\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"INLINE uint32_t elemNum(const scalar_dw &x) { return 1; }\n"
"INLINE uint32_t elemNum(const scalar_w &x) { return 1; }\n"
"INLINE uint32_t elemNum(const scalar_m &x) { return 1; }\n"
"INLINE uint32_t mask(const scalar_m &v) { return v.u ? 1 : 0; }\n"
"\n"
"// 32 bit floating points\n"
"INLINE void ADD_F(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.f = v0.f + v1.f; }\n"
"INLINE void SUB_F(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.f = v0.f - v1.f; }\n"
"INLINE void MUL_F(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.f = v0.f * v1.f; }\n"
"INLINE void DIV_F(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.f = v0.f / v1.f; }\n"
"INLINE void EQ_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f == v1.f ? ~0 : 0); }\n"
"INLINE void NE_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f != v1.f ? ~0 : 0); }\n"
"INLINE void LE_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f <= v1.f ? ~0 : 0); }\n"
"INLINE void LT_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f < v1.f ? ~0 : 0); }\n"
"INLINE void GE_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f >= v1.f ? ~0 : 0); }\n"
"INLINE void GT_F(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.f > v1.f ? ~0 : 0); }\n"
"\n"
"// 32 bit integers\n"
"INLINE void ADD_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s + v1.s; }\n"
"INLINE void SUB_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s - v1.s; }\n"
"INLINE void MUL_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s * v1.s; }\n"
"INLINE void DIV_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s / v1.s; }\n"
"INLINE void REM_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s % v1.s; }\n"
"INLINE void MUL_U32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.u = v0.u * v1.u; }\n"
"INLINE void DIV_U32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.u = v0.u / v1.u; }\n"
"INLINE void REM_U32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.u = v0.u % v1.u; }\n"
"INLINE void EQ_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s == v1.s ? ~0 : 0); }\n"
"INLINE void NE_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s != v1.s ? ~0 : 0); }\n"
"INLINE void LE_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s <= v1.s ? ~0 : 0); }\n"
"INLINE void LT_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s < v1.s ? ~0 : 0); }\n"
"INLINE void GE_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s >= v1.s ? ~0 : 0); }\n"
"INLINE void GT_S32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.s > v1.s ? ~0 : 0); }\n"
"INLINE void XOR_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s ^ v1.s; }\n"
"INLINE void OR_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s | v1.s; }\n"
"INLINE void AND_S32(scalar_dw &dst, scalar_dw v0, scalar_dw v1) { dst.s = v0.s & v1.s; }\n"
"INLINE void LE_U32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.u <= v1.u ? ~0 : 0); }\n"
"INLINE void LT_U32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.u < v1.u ? ~0 : 0); }\n"
"INLINE void GE_U32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.u >= v1.u ? ~0 : 0); }\n"
"INLINE void GT_U32(scalar_m &dst, scalar_dw v0, scalar_dw v1) { dst.u = (v0.u > v1.u ? ~0 : 0); }\n"
"INLINE void LOAD(scalar_dw &dst, const char *ptr) { dst.u = *(const uint32_t *) ptr; }\n"
"INLINE void STORE(scalar_dw src, char *ptr) { *(uint32_t *) ptr = src.u; }\n"
"INLINE void LOADI(scalar_dw &dst, uint32_t u) { dst.u = u; }\n"
"INLINE void SCATTER(scalar_dw offset, scalar_dw value, char *base) { *(uint32_t*)(base + offset.u) = value.u; }\n"
"INLINE void GATHER(scalar_dw &dst, scalar_dw offset, const char *base) { dst.u = *(const uint32_t*)(base + offset.u); }\n"
"\n"
"// 16 bit floating points\n"
"INLINE void ADD_U16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.u = v0.u + v1.u; }\n"
"INLINE void SUB_U16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.u = v0.u - v1.u; }\n"
"INLINE void ADD_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s + v1.s; }\n"
"INLINE void SUB_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s - v1.s; }\n"
"INLINE void MUL_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s * v1.s; }\n"
"INLINE void DIV_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s / v1.s; }\n"
"INLINE void REM_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s % v1.s; }\n"
"INLINE void MUL_U16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.u = v0.u * v1.u; }\n"
"INLINE void DIV_U16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.u = v0.u / v1.u; }\n"
"INLINE void REM_U16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.u = v0.u % v1.u; }\n"
"INLINE void EQ_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s == v1.s ? ~0 : 0); }\n"
"INLINE void NE_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s != v1.s ? ~0 : 0); }\n"
"INLINE void LE_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s <= v1.s ? ~0 : 0); }\n"
"INLINE void LT_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s < v1.s ? ~0 : 0); }\n"
"INLINE void GE_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s >= v1.s ? ~0 : 0); }\n"
"INLINE void GT_S16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.s > v1.s ? ~0 : 0); }\n"
"INLINE void XOR_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s ^ v1.s; }\n"
"INLINE void OR_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s | v1.s; }\n"
"INLINE void AND_S16(scalar_w &dst, scalar_w v0, scalar_w v1) { dst.s = v0.s & v1.s; }\n"
"INLINE void LE_U16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.u <= v1.u ? ~0 : 0); }\n"
"INLINE void LT_U16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.u < v1.u ? ~0 : 0); }\n"
"INLINE void GE_U16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.u >= v1.u ? ~0 : 0); }\n"
"INLINE void GT_U16(scalar_m &dst, scalar_w v0, scalar_w v1) { dst.u = (v0.u > v1.u ? ~0 : 0); }\n"
"INLINE void LOAD(scalar_w &dst, const char *ptr) { dst.u = *(const uint16_t *) ptr; }\n"
"INLINE void STORE(scalar_w src, char *ptr) { *(uint16_t *) ptr = src.u; }\n"
"INLINE void LOADI(scalar_w &dst, uint16_t u) { dst.u = u; }\n"
"INLINE void SCATTER(scalar_w offset, scalar_w value, char *base) { *(uint16_t*)(base + offset.u) = value.u; }\n"
"INLINE void GATHER(scalar_w &dst, scalar_w offset, const char *base) { dst.u = *(const uint16_t*)(base + offset.u); }\n"
"\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"// Identical instructions are forwarded\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"\n"
"// Forward identical 32 bit instructions\n"
"#define NOV_U32 MOV_S32\n"
"#define NOV_F MOV_S32\n"
"#define ADD_U32 ADD_S32\n"
"#define SUB_U32 SUB_S32\n"
"#define XOR_U32 XOR_S32\n"
"#define OR_U32 OR_S32\n"
"#define AND_U32 AND_S32\n"
"#define EQ_U32 EQ_S32\n"
"#define NE_U32 NE_S32\n"
"\n"
"// Forward identical 16 bit instructions\n"
"#define NOV_U16 MOV_S16\n"
"#define ADD_U16 ADD_S16\n"
"#define SUB_U16 SUB_S16\n"
"#define AND_U16 AND_S16\n"
"#define XOR_U16 XOR_S16\n"
"#define OR_U16 OR_S16\n"
"#define AND_U16 AND_S16\n"
"#define EQ_U16 EQ_S16\n"
"#define NE_U16 NE_S16\n"
"\n"
"#undef PS2SI\n"
"#undef SI2PS\n"
"#undef ID\n"
"\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"// Goto implementation which is directly inspired by BDW goto and by this\n"
"// article \"Whole function vectorization\" (CGO 2011)\n"
"//////////////////////////////////////////////////////////////////////////////\n"
"\n"
"/*! Update the UIP vector according for the lanes alive in mask */\n"
"template <uint32_t vectorNum>\n"
"void updateUIP(simd_w<vectorNum> &uipVec, const simd_m<vectorNum> mask, uint16_t uip) {\n"
"  union { float f; uint32_t u; } x;\n"
"  x.u = uip;\n"
"  __m128 v = _mm_load1_ps(&x.f);\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    uipVec.m[i] = _mm_blendv_ps(uipVec.m[i], v, mask.m[i]);\n"
"}\n"
"\n"
"/*! Update the UIP vector according for the lanes alive in mask */\n"
"template <uint32_t vectorNum>\n"
"void updateUIPC(simd_w<vectorNum> &uipVec,\n"
"                const simd_m<vectorNum> mask,\n"
"                const simd_m<vectorNum> cond,\n"
"                uint16_t uip) {\n"
"  union { float f; uint32_t u; } x;\n"
"  x.u = uip;\n"
"  __m128 v = _mm_load1_ps(&x.f);\n"
"  for (uint32_t i = 0; i < vectorNum; ++i)\n"
"    uipVec.m[i] = _mm_blendv_ps(uipVec.m[i], v, _mm_and_ps(cond.m[i], mask.m[i]));\n"
"}\n"
"\n"
"/*! Update the execution mask based on block IP and UIP values */\n"
"template <uint32_t vectorNum>\n"
"void updateMask(simd_m<vectorNum> &mask, const simd_w<vectorNum> &uipVec, uint16_t ip) {\n"
"  const simd_w<vectorNum> ipv(ip);\n"
"  LE_U16(mask, uipVec, ipv);\n"
"}\n"
"\n"
"/*! Jump to the block JIP */\n"
"#define SIM_FWD_BRA(UIPVEC, EMASK, JIP, UIP) \\\n"
"  do { \\\n"
"    updateUIP(UIPVEC, EMASK, UIP); \\\n"
"    goto label##JIP; \\\n"
"  } while (0)\n"
"\n"
"/*! Based on the condition jump to block JIP */\n"
"#define SIM_FWD_BRA_C(UIPVEC, EMASK, COND, JIP, UIP) \\\n"
"  do { \\\n"
"    updateUIPC(UIPVEC, EMASK, COND, UIP); \\\n"
"    typeof(COND) jumpCond; \\\n"
"    scalar_w jipScalar(uint16_t(JIP)); \\\n"
"    LT_U16(jumpCond, UIPVEC, uint16_t(JIP)); \\\n"
"    uint32_t jumpMask = mask(jumpCond); \\\n"
"    if (!jumpMask) goto label##JIP; \\\n"
"  } while (0)\n"
"\n"
"/*! Backward jump is always taken */\n"
"#define SIM_BWD_BRA(UIPVEC, EMASK, JIP) \\\n"
"  do { \\\n"
"    updateUIP(UIPVEC, EMASK, JIP); \\\n"
"    goto label##JIP; \\\n"
"  } while (0)\n"
"\n"
"/*! Conditional backward jump is taken if the condition is non-null */\n"
"#define SIM_BWD_BRA_C(UIPVEC, EMASK, COND, JIP) \\\n"
"  do { \\\n"
"    updateUIPC(UIPVEC, EMASK, COND, JIP); \\\n"
"    typeof(COND) JUMP_MASK; \\\n"
"    AND_M(JUMP_MASK, COND, EMASK); \\\n"
"    if (mask(JUMP_MASK) != 0) goto label##JIP; \\\n"
"  } while (0)\n"
"\n"
"/*! JOIN: reactivates lanes */\n"
"#define SIM_JOIN(UIPVEC, MASK, IP) \\\n"
"  do { \\\n"
"    updateMask(MASK, UIPVEC, IP); \\\n"
"    movedMask = mask(MASK); \\\n"
"  } while (0)\n"
"\n"
"/*! JOIN_JUMP: ractivate lanes and jump to JIP if none is activated */\n"
"#define SIM_JOIN_JUMP(UIPVEC, EMASK, IP, JIP) \\\n"
"  do { \\\n"
"    SIM_JOIN(UIPVEC, EMASK, IP); \\\n"
"    const uint32_t execMask = mask(EMASK); \\\n"
"    if (execMask == 0) goto label##JIP; \\\n"
"  } while (0)\n"
"\n"
"/* Macro to apply masking on destinations (from zero to four destinations) */\n"
"#define MASKED0(OP, ...) \\\n"
"  do { \\\n"
"    OP(__VA_ARGS__); \\\n"
"  } while (0)\n"
"\n"
"#define MASKED1(OP, ARG0, ...) \\\n"
"  do { \\\n"
"    typeof(ARG0) ARG0##__; \\\n"
"    OP(ARG0##__, __VA_ARGS__); \\\n"
"    select(ARG0, ARG0, ARG0##__, emask); \\\n"
"  } while (0)\n"
"\n"
"#define MASKED2(OP, ARG0, ARG1, ...) \\\n"
"  do { \\\n"
"    typeof(ARG0) ARG0##__; \\\n"
"    typeof(ARG1) ARG1##__; \\\n"
"    OP(ARG0##__, ARG1##__, __VA_ARGS__); \\\n"
"    select(ARG0, ARG0, ARG0##__, emask); \\\n"
"    select(ARG1, ARG1, ARG1##__, emask); \\\n"
"  } while (0)\n"
"\n"
"#define MASKED3(OP, ARG0, ARG1, ARG2, ...) \\\n"
"  do { \\\n"
"    typeof(ARG0) ARG0##__; \\\n"
"    typeof(ARG1) ARG1##__; \\\n"
"    typeof(ARG2) ARG2##__; \\\n"
"    OP(ARG0##__, ARG1##__, ARG2##__, __VA_ARGS__); \\\n"
"    select(ARG0, ARG0, ARG0##__, emask); \\\n"
"    select(ARG1, ARG1, ARG1##__, emask); \\\n"
"    select(ARG2, ARG2, ARG2##__, emask); \\\n"
"  } while (0)\n"
"\n"
"#define MASKED4(OP, ARG0, ARG1, ARG2, ARG3, ...) \\\n"
"  do { \\\n"
"    typeof(ARG0) ARG0##__; \\\n"
"    typeof(ARG1) ARG1##__; \\\n"
"    typeof(ARG2) ARG2##__; \\\n"
"    typeof(ARG3) ARG3##__; \\\n"
"    OP(ARG0##__, ARG1##__, ARG2##__, ARG3##__, __VA_ARGS__); \\\n"
"    select(ARG0, ARG0, ARG0##__, emask); \\\n"
"    select(ARG1, ARG1, ARG1##__, emask); \\\n"
"    select(ARG2, ARG2, ARG2##__, emask); \\\n"
"    select(ARG3, ARG3, ARG3##__, emask); \\\n"
"  } while (0)\n"
"\n"
"#undef INLINE\n"
"\n"
"#endif /* __GBE_SIM_VECTOR_H__ */\n"
"\n"
;
}

